---
title: "Step-by-Step Guide to GPU Setup for Machine Learning & Deep Learning!"
datePublished: Wed Jan 15 2025 21:03:15 GMT+0000 (Coordinated Universal Time)
cuid: cm5ye2c17000309mm82w12rm2
slug: step-by-step-guide-to-gpu-setup-for-machine-learning-deep-learning
tags: ai, python, ml, jupyter-notebook, gpu

---

Hello everyone, and welcome to this tutorial on how to use your GPU for AI tasks, including machine learning and deep learning, instead of relying on your CPU. Using a GPU can significantly accelerate computations, especially for deep learning models. Let's get started step by step.

---

**Step 1: Create a New Environment and Install Required Packages**

First, open the Anaconda Prompt and create a new environment with TensorFlow GPU support. Use the following command:

`conda create –n tf-GPU tensorflow-gpu`

This will create a new environment named tf-GPU with the necessary TensorFlow GPU version installed.

---

**Step 2: Activate the New Environment**

Activate the newly created environment by typing:

`conda activate tf-GPU`

This ensures all your installations and tasks are isolated to this environment.

---

**Step 3: Install TensorFlow 2.10**

TensorFlow 2.10 is compatible with many GPUs. Install it using:

`python -m pip install "tensorflow==2.10.0"`

---

**Step 4: Install the Jupyter Kernel for This Environment**

To use this environment in Jupyter Notebook, run the following commands:

`pip install ipykernel`

`python -m ipykernel install --user --name=tf-GPU --display-name "Python (tf-GPU)"`

This makes the tf-GPU environment available as a kernel in Jupyter Notebook.

---

**Step 5: Open Jupyter Notebook**

Now, open Jupyter Notebook:

`jupyter notebook`

In Jupyter, go to the **Kernel** menu, select **Change Kernel**, and choose **Python (tf-GPU)**.

---

**Step 6: Verify GPU Availability in Jupyter Notebook**

To check if TensorFlow recognizes your GPU, run the following code:

`import tensorflow as tf`

`print("GPU Details:", tf.config.list_physical_devices('GPU'))`

This should display details of your GPU.

---

**Step 7: First Example - Comparing CPU and GPU Performance**

Here’s a simple example to compare the time taken for matrix multiplication on GPU and CPU:

```python
import tensorflow as tf
import time

# Check available GPU
print("Available GPU:", tf.config.list_physical_devices('GPU'))

# Create large random matrices
matrix_size = 10000
a = tf.random.normal((matrix_size, matrix_size))
b = tf.random.normal((matrix_size, matrix_size))

# Perform matrix multiplication on GPU
start_time = time.time()
c = tf.matmul(a, b)
end_time = time.time()
print(f"Matrix multiplication on GPU completed in {end_time - start_time:.4f} seconds")

# Perform matrix multiplication on CPU
with tf.device('/CPU:0'):
    start_time = time.time()
    c_cpu = tf.matmul(a, b)
    end_time = time.time()
    print(f"Matrix multiplication on CPU completed in {end_time - start_time:.4f} seconds")
```

---

**Step 8: Second Example - Training a Neural Network Using the GPU**

Let’s now train a simple neural network on the MNIST dataset using the GPU:

```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the data (scale pixel values to 0-1)
x_train, x_test = x_train / 255.0, x_test / 255.0

# Build a simple neural network model
model = Sequential([
    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images into 1D vectors
    Dense(128, activation='relu'), # First dense layer with 128 units
    Dense(10, activation='softmax') # Output layer with 10 classes (digits 0-9)
])

# Compile the model
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)
print(f"Test Accuracy: {test_accuracy:.4f}")
```

**Summary**

In this tutorial, we learned how to set up a GPU-enabled TensorFlow environment, verify GPU usage, and perform computations using TensorFlow. We also compared GPU and CPU performance and trained a simple neural network using the GPU. By leveraging GPUs, you can significantly speed up your AI tasks and focus on building better models faster.

For a more comprehensive learning experience, check out my Udemy course: [AI for Geospatial Analysis](https://www.udemy.com/course/ai-for-geospatial-analysis/?referralCode=756925DC2106C34C6DBE). This course dives deeper into AI and machine learning applications for geospatial data analysis.